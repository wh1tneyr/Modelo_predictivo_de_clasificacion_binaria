{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introducción"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el siguiente proyecto estaré desarrollando un modelo de aprendizaje para un banco que está experimentando una creciente tasa de abandono por parte de sus clientes cada mes.  Para el banco es más económico salvar a los clientes que permanecen, que atrer a nuevos. \n",
    "\n",
    "Mi objetivo es crear un modelo de clasificación que me permita predecir exitosamente si un cliente abandonará el banco pronto. Para cumplir con esa tarea, estaré trabajando con la base de datos proporcionada por el banco, la cual me permitirá evaluar el comportamiento pasado de los clientes y la terminación de contratos con el banco.\n",
    "\n",
    "Mi enfoque en cuanto a las predicciones de si un cliente abandonará o no, será disminuir el máximo posible de falsos negativos y falsos positivos; para lo cual me concentraré en la media armónica de las métricas de precisión y sensibilidad.\n",
    "\n",
    "#### Tabla de contenido:\n",
    "\n",
    "1. Inicialización: importar las librerías.\n",
    "  - 1.2. Cargar y preparar los datos. \n",
    "2. Codificación de los datos categóricos:\n",
    "  - 2.2. Codificación OHE, segmentación de conjuntos de datos y estandarización de características numéricas para modelos de regresión.\n",
    "  - 2.3. Codificación de etiquetas, segmentación de conjuntos de datos y estandarización de características numéricas  para modelos basados en árboles.\n",
    "3. Entrenar los modelos.\n",
    "  - 3.2. Examinar el equilibrio de clases.\n",
    "4. Elegir un modelo y comprobar su calidad con el conjunto de prueba.\n",
    "5. Prueba de cordura en el modelo elegido.\n",
    "6. Conclusiones generales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Inicialización"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importar librerías\n",
    "import pandas as pd\n",
    "from sklearn.metrics import recall_score, precision_score, confusion_matrix, f1_score, roc_auc_score\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OrdinalEncoder\n",
    "from sklearn.utils import shuffle\n",
    "import matplotlib as plt\n",
    "\n",
    "#librerias para los modelos \n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Cargar y preparar los datos "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cargar el dataset en un dataframe\n",
    "data = pd.read_csv('dataset/Churn.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10000 entries, 0 to 9999\n",
      "Data columns (total 14 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   RowNumber        10000 non-null  int64  \n",
      " 1   CustomerId       10000 non-null  int64  \n",
      " 2   Surname          10000 non-null  object \n",
      " 3   CreditScore      10000 non-null  int64  \n",
      " 4   Geography        10000 non-null  object \n",
      " 5   Gender           10000 non-null  object \n",
      " 6   Age              10000 non-null  int64  \n",
      " 7   Tenure           9091 non-null   float64\n",
      " 8   Balance          10000 non-null  float64\n",
      " 9   NumOfProducts    10000 non-null  int64  \n",
      " 10  HasCrCard        10000 non-null  int64  \n",
      " 11  IsActiveMember   10000 non-null  int64  \n",
      " 12  EstimatedSalary  10000 non-null  float64\n",
      " 13  Exited           10000 non-null  int64  \n",
      "dtypes: float64(3), int64(8), object(3)\n",
      "memory usage: 1.1+ MB\n"
     ]
    }
   ],
   "source": [
    "#visualizar información general del dataframe\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RowNumber</th>\n",
       "      <th>CustomerId</th>\n",
       "      <th>Surname</th>\n",
       "      <th>CreditScore</th>\n",
       "      <th>Geography</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>Tenure</th>\n",
       "      <th>Balance</th>\n",
       "      <th>NumOfProducts</th>\n",
       "      <th>HasCrCard</th>\n",
       "      <th>IsActiveMember</th>\n",
       "      <th>EstimatedSalary</th>\n",
       "      <th>Exited</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>15634602</td>\n",
       "      <td>Hargrave</td>\n",
       "      <td>619</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>42</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>101348.88</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>15647311</td>\n",
       "      <td>Hill</td>\n",
       "      <td>608</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Female</td>\n",
       "      <td>41</td>\n",
       "      <td>1.0</td>\n",
       "      <td>83807.86</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>112542.58</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>15619304</td>\n",
       "      <td>Onio</td>\n",
       "      <td>502</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>42</td>\n",
       "      <td>8.0</td>\n",
       "      <td>159660.80</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113931.57</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>15701354</td>\n",
       "      <td>Boni</td>\n",
       "      <td>699</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>39</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>93826.63</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>15737888</td>\n",
       "      <td>Mitchell</td>\n",
       "      <td>850</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Female</td>\n",
       "      <td>43</td>\n",
       "      <td>2.0</td>\n",
       "      <td>125510.82</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>79084.10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   RowNumber  CustomerId   Surname  CreditScore Geography  Gender  Age  \\\n",
       "0          1    15634602  Hargrave          619    France  Female   42   \n",
       "1          2    15647311      Hill          608     Spain  Female   41   \n",
       "2          3    15619304      Onio          502    France  Female   42   \n",
       "3          4    15701354      Boni          699    France  Female   39   \n",
       "4          5    15737888  Mitchell          850     Spain  Female   43   \n",
       "\n",
       "   Tenure    Balance  NumOfProducts  HasCrCard  IsActiveMember  \\\n",
       "0     2.0       0.00              1          1               1   \n",
       "1     1.0   83807.86              1          0               1   \n",
       "2     8.0  159660.80              3          1               0   \n",
       "3     1.0       0.00              2          0               0   \n",
       "4     2.0  125510.82              1          1               1   \n",
       "\n",
       "   EstimatedSalary  Exited  \n",
       "0        101348.88       1  \n",
       "1        112542.58       0  \n",
       "2        113931.57       1  \n",
       "3         93826.63       0  \n",
       "4         79084.10       0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#visualizar los datos\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En cuanto a los tipos de datos de cada columna, parecen correctos para cada categoría correspondiente. Sin embargo, para mi tarea de clasificación debo codificar más adelante algunas columnas categóricas; estas columnas serán: 'Surname', 'Geography' y 'Gender'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#verificar los duplicados en el dataframe\n",
    "data.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RowNumber            0\n",
       "CustomerId           0\n",
       "Surname              0\n",
       "CreditScore          0\n",
       "Geography            0\n",
       "Gender               0\n",
       "Age                  0\n",
       "Tenure             909\n",
       "Balance              0\n",
       "NumOfProducts        0\n",
       "HasCrCard            0\n",
       "IsActiveMember       0\n",
       "EstimatedSalary      0\n",
       "Exited               0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#verificar datos ausentes\n",
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "He encontrado algunos datos ausentes en la columna 'Tenure'. Esta columna contiene el período durante el cual ha madurado el depósito a plazo fijo de un cliente (años).\n",
    "En vista de que esa información pudiera ser importante para que el modelo pueda predecir si un cliente abandonará el banco, procederé a rellenar los datos ausentes con el período mediano, agrupando por país y género."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RowNumber          0\n",
       "CustomerId         0\n",
       "Surname            0\n",
       "CreditScore        0\n",
       "Geography          0\n",
       "Gender             0\n",
       "Age                0\n",
       "Tenure             0\n",
       "Balance            0\n",
       "NumOfProducts      0\n",
       "HasCrCard          0\n",
       "IsActiveMember     0\n",
       "EstimatedSalary    0\n",
       "Exited             0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#rellenar los datos ausentes en la columna 'Tenure'\n",
    "data['Tenure'] = data['Tenure'].fillna(data.groupby(['Geography', 'Gender'])['Tenure'].transform('median'))\n",
    "\n",
    "#verificar nuevamente los datos ausentes\n",
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Eliminar columnas innecesarias del DataFrame\n",
    "set_data = data.drop(['RowNumber', 'CustomerId', 'Surname'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparé un DataFrame, eliminando los datos ausentes y las columnas innecesarias para realizar las predicciones.\n",
    "Los datos ya están preparados para pasar a la segmentación de los conjuntos de entrenamiento, validación y prueba."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Codificación de columnas categóricas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Codificación OHE de columnas categóricas para los modelos de regresión"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamaños de los conjuntos de datos OHE:\n",
      "features train ohe: (6000, 11)\n",
      "target train ohe: (6000,)\n",
      "\n",
      "Conjuntos de validación:\n",
      "features valid ohe: (2400, 11)\n",
      "target_valid ohe: (2400,)\n",
      "\n",
      "Conjuntos de prueba:\n",
      "features test ohe: (1600, 11)\n",
      "target test ohe: (1600,)\n"
     ]
    }
   ],
   "source": [
    "#codificación OHE para las regresiones\n",
    "data_ohe = pd.get_dummies(set_data, drop_first=True)\n",
    "\n",
    "#definir el objetivo y las features\n",
    "target_ohe = data_ohe['Exited']\n",
    "features_ohe = data_ohe.drop(['Exited'], axis=1)\n",
    "\n",
    "#segmentación de los datos para conjuntos de entrenamiento y validación ohe\n",
    "features_train_ohe, features_valid_ohe, target_train_ohe, target_valid_ohe = train_test_split(features_ohe, target_ohe, test_size=0.40, random_state=12345)\n",
    "\n",
    "#segmentacion del conjunto de validacion para conjunto de prueba ohe\n",
    "features_valid_ohe, features_test_ohe, target_valid_ohe, target_test_ohe = train_test_split(features_valid_ohe, target_valid_ohe, test_size=0.40, random_state=12345)\n",
    "\n",
    "#verificando los tamaños de los conjuntos \n",
    "print('Tamaños de los conjuntos de datos OHE:')\n",
    "print('features train ohe:', features_train_ohe.shape)\n",
    "print('target train ohe:', target_train_ohe.shape)\n",
    "print()\n",
    "print('Conjuntos de validación:')\n",
    "print('features valid ohe:', features_valid_ohe.shape)\n",
    "print('target_valid ohe:', target_valid_ohe.shape)\n",
    "print()\n",
    "print('Conjuntos de prueba:')\n",
    "print('features test ohe:', features_test_ohe.shape)\n",
    "print('target test ohe:', target_test_ohe.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para los modelos de regresión, he codificado las columnas categóricas del DataFrame utilizando una codificación One-Hot. A continuación, segmenté el dataset baset en tres conjuntos: entrenamiento, validación y prueba, en una proporción de 6:4.4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Codificación de etiquetas para modelos basados en árboles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamaños de los conjuntos de datos:\n",
      "features train ordinal: (6000, 10)\n",
      "target train ordinal: (6000,)\n",
      "\n",
      "features valid ordinal: (2400, 10)\n",
      "target valid ordinal: (2400,)\n",
      "\n",
      "features test ordinal: (1600, 10)\n",
      "target test ordinal: (1600,)\n"
     ]
    }
   ],
   "source": [
    "#ordinal encoder para modelos basados en árboles\n",
    "encoder = OrdinalEncoder()\n",
    "data_ordinal = pd.DataFrame(encoder.fit_transform(set_data), columns=set_data.columns)\n",
    "\n",
    "#definir el objetivo y las features \n",
    "target_ordinal = data_ordinal['Exited']\n",
    "features_ordinal = data_ordinal.drop(['Exited'], axis=1)\n",
    "\n",
    "#segmentar los datos en conjuntos de entrenamiento y validcación\n",
    "features_train_ordinal, features_valid_ordinal, target_train_ordinal, target_valid_ordinal = train_test_split(features_ordinal, target_ordinal, test_size=0.40, random_state=12345)\n",
    "\n",
    "#segmentar el conjunto de validación en un subconjunto de prueba\n",
    "features_valid_ordinal, features_test_ordinal, target_valid_ordinal, target_test_ordinal = train_test_split(features_valid_ordinal, target_valid_ordinal, test_size=0.40, random_state=12345)\n",
    "\n",
    "\n",
    "print('Tamaños de los conjuntos de datos:')\n",
    "print('features train ordinal:' , features_train_ordinal.shape)\n",
    "print('target train ordinal:' , target_train_ordinal.shape)\n",
    "print()\n",
    "print('features valid ordinal:' , features_valid_ordinal.shape)\n",
    "print('target valid ordinal:' , target_valid_ordinal.shape)\n",
    "print()\n",
    "print('features test ordinal:', features_test_ordinal.shape)\n",
    "print('target test ordinal:', target_test_ordinal.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para los modelos basados en árboles que voy a entrenar, decidí codificar las columnas categóricas utilizando la codificación de etiquetas, más especificamente, el OrdinalEncoder. Luego, también segmenté el dataset base en tres conjuntos: entrenamiento, validación y prueba, en una proporción 6:4:4. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Examinar el equilibrio de clases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conteo de objetivos de clase positiva (1): 2037\n",
      "Conteo de objetivos de clase negativa (0)\": 7963\n"
     ]
    }
   ],
   "source": [
    "#examinar el equilibrio de clases \n",
    "print('Conteo de objetivos de clase positiva (1):', (set_data['Exited'] == 1).sum())\n",
    "print('Conteo de objetivos de clase negativa (0)\":', (set_data['Exited'] == 0).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al examinar la variable objetivo, se observa un fuerte desequilibrio entre la clase 1 y la clase 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Entrenar modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. Modelo basado en árboles de decisión"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mejor modelo de árbol de decisión: None\n",
      "Mejor valor f1 para árbol de decisión: 0.59\n"
     ]
    }
   ],
   "source": [
    "#conjuntos de entrenamiento \n",
    "features_train_ordinal\n",
    "target_train_ordinal\n",
    "\n",
    "#conjuntos de validación\n",
    "features_valid_ordinal\n",
    "target_valid_ordinal\n",
    "\n",
    "#conjuntos de prueba\n",
    "features_test_ordinal\n",
    "target_test_ordinal\n",
    "\n",
    "#arbol de decision\n",
    "best_model_dt = None\n",
    "best_depth_dt = 0\n",
    "f1_best_result_dt = 0.59\n",
    "\n",
    "for depth in range (1, 10):\n",
    "    model_tree = DecisionTreeClassifier(\n",
    "        random_state=12345, max_depth=depth, min_samples_split=3, min_samples_leaf=3\n",
    "        )\n",
    "    model_tree.fit(features_train_ordinal, target_train_ordinal)\n",
    "    \n",
    "    predicted_valid = model_tree.predict(features_valid_ordinal)\n",
    "    result = f1_score(target_valid_ordinal, predicted_valid)\n",
    "\n",
    "    if result >= f1_best_result_dt:\n",
    "        best_model_dt = model_tree\n",
    "        f1_best_result_dt = result\n",
    "        best_depth_dt = depth\n",
    "\n",
    "\n",
    "\n",
    "print('Mejor modelo de árbol de decisión:', best_model_dt)\n",
    "print('Mejor valor f1 para árbol de decisión:', f1_best_result_dt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.. Modelo de bosque aleatorio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mejor modelo de bosque aleatorio: None\n",
      "\n",
      "Mejor valor f1 para bosque aleatorio: 0.59\n"
     ]
    }
   ],
   "source": [
    "#conjuntos de entrenamiento \n",
    "features_train_ordinal\n",
    "target_train_ordinal\n",
    "\n",
    "#conjuntos de validación\n",
    "features_valid_ordinal\n",
    "target_valid_ordinal\n",
    "\n",
    "#conjuntos de prueba\n",
    "features_test_ordinal\n",
    "target_test_ordinal\n",
    "\n",
    "#bosque aleatorio\n",
    "best_model_f = None\n",
    "f1_best_result_f = 0.59\n",
    "best_depth_f = 0\n",
    "best_est_f = 0\n",
    "\n",
    "for est in range(10, 80, 100):\n",
    "    for depth in range(1, 20):\n",
    "        model_forest = RandomForestClassifier(\n",
    "            random_state=54321, n_estimators=est, max_depth=depth, min_samples_leaf=3, \n",
    "            min_samples_split=2, max_features=5\n",
    "            )\n",
    "        model_forest.fit(features_train_ordinal, target_train_ordinal)\n",
    "        predicted_valid_forest = model_forest.predict(features_valid_ordinal)\n",
    "        \n",
    "        result = f1_score(target_valid_ordinal, predicted_valid_forest)\n",
    "\n",
    "        if result >= f1_best_result_f:\n",
    "            best_model_f = model_forest\n",
    "            f1_best_result_f = result\n",
    "            best_depth_f = depth\n",
    "            best_est_f = est\n",
    "\n",
    "print('Mejor modelo de bosque aleatorio:', best_model_f)\n",
    "print()\n",
    "print('Mejor valor f1 para bosque aleatorio:', f1_best_result_f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3. Modelo de regresión logística"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valor f1 para regresión logística: 0.09540636042402827\n"
     ]
    }
   ],
   "source": [
    "#Conjuntos de entrenamiento\n",
    "features_train_ohe\n",
    "target_train_ohe\n",
    "\n",
    "#Conjuntos de validación\n",
    "features_valid_ohe\n",
    "target_valid_ohe\n",
    "\n",
    "#Conjunto de prueba \n",
    "features_test_ohe\n",
    "target_test_ohe\n",
    "\n",
    "#regresión logística\n",
    "model_logistic_regre = LogisticRegression(random_state=12345, solver='liblinear', max_iter=800, penalty='l2')\n",
    "model_logistic_regre.fit(features_train_ohe, target_train_ohe)\n",
    "\n",
    "predicted_valid_logistic_regre = model_logistic_regre.predict(features_valid_ohe)\n",
    "\n",
    "f1_logistic_regresion = f1_score(target_valid_ohe, predicted_valid_logistic_regre)\n",
    "\n",
    "print('Valor f1 para regresión logística:', f1_logistic_regresion)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "He entrenado tres modelos de clasificación: árboles de decisión, bosque aleatorio y regresión logística. Al momento de entrenarlos no he tomado en cuenta el desequilibrio de clases existente, por lo tanto, ninguno de los modelos me ha proporcionado un valor F1 suficientemente alto (de al menos 0.59), como para que resulten útiles en mi tarea..\n",
    "A continuación utilizaré dos enfoques para equilibrar las clases y mejoraré la calidad de los modelos. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Corregir el desequilibrio de clases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1. Sobremuestreo (upsample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sobremuestreo (upsample)\n",
    "def upsample(features, target, repeat):\n",
    "    features_zeros = features[target == 0]\n",
    "    features_ones = features[target == 1]\n",
    "    target_zeros = target[target == 0]\n",
    "    target_ones = target[target == 1]\n",
    "\n",
    "    repeat = 10\n",
    "    features_upsampled = pd.concat([features_zeros] + [features_ones] * repeat)\n",
    "    target_upsampled = pd.concat([target_zeros] + [target_ones] * repeat)\n",
    "\n",
    "    features_upsampled, target_upsampled = shuffle(features_upsampled, target_upsampled, random_state=12345)\n",
    "\n",
    "    return features_upsampled, target_upsampled\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#definir variables sobremuestreadas para modelos de arboles\n",
    "features_train_upsampled_trees, target_train_upsampled_trees = upsample(features_train_ordinal, target_train_ordinal, 10)\n",
    "\n",
    "#definir variables sobremuestreadas para modelo de regresión\n",
    "features_train_upsampled_regre, target_train_upsampled_regre = upsample(features_train_ohe, target_train_ohe, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "He definido cuatro variables de entrenamiento sobremuestreadas, para los modelos de árboles y el de regresión respectivamente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2. Submuestreo (downsample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#submuestreo (downsample)\n",
    "def downsample(features, target, fraction):\n",
    "    features_zeros = features[target == 0]\n",
    "    features_ones = features[target == 1]\n",
    "    target_zeros = target[target == 0]\n",
    "    target_ones = target[target == 1]\n",
    "\n",
    "    features_downsampled = pd.concat([features_zeros.sample(frac=fraction, random_state=12345)] + [features_ones])\n",
    "    target_downsampled = pd.concat([target_zeros.sample(frac=fraction, random_state=12345)] + [target_ones])\n",
    "\n",
    "    features_upsampled, target_upsampled = shuffle(features_downsampled, target_downsampled, random_state=12345)\n",
    "\n",
    "    return features_downsampled, target_downsampled\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#definir variables submuestreadas para modelos de arboles\n",
    "features_train_downsampled_trees, target_train_downsampled_trees = downsample(features_train_ordinal, target_train_ordinal, 0.1)\n",
    "\n",
    "#definir variables submuestreadas para modelo de regresión\n",
    "features_train_downsampled_regre, target_train_downsampled_regre = downsample(features_train_ohe, target_train_ohe, 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Mejorar la calidad de los modelos con clases equilibradas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Árbol de decisión"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mejor modelo de árbol de decisión: DecisionTreeClassifier(class_weight='balanced', max_depth=5, min_samples_leaf=3,\n",
      "                       min_samples_split=3, random_state=12345)\n",
      "Mejor valor f1 para árbol de decisión: 0.5884476534296029\n"
     ]
    }
   ],
   "source": [
    "#modelo basado en árboles de decisión con clases equilibradas\n",
    "\n",
    "best_model_tree = None\n",
    "f1_best_result_tree = 0.30\n",
    "\n",
    "for depth in range (1, 10):\n",
    "    model_tree = DecisionTreeClassifier(\n",
    "        random_state=12345, max_depth=depth, min_samples_split=3, min_samples_leaf=3, class_weight='balanced'\n",
    "        )\n",
    "    model_tree.fit(features_train_upsampled_trees, target_train_upsampled_trees)\n",
    "    \n",
    "    predicted_valid = model_tree.predict(features_valid_ordinal)\n",
    "    result = f1_score(target_valid_ordinal, predicted_valid)\n",
    "\n",
    "    if result >= f1_best_result_tree:\n",
    "        best_model_tree = model_tree\n",
    "        f1_best_result_tree = result\n",
    "    \n",
    "\n",
    "print('Mejor modelo de árbol de decisión:', best_model_tree)\n",
    "print('Mejor valor f1 para árbol de decisión:', f1_best_result_tree)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2. Bosque aleatorio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mejor modelo de bosque aleatorio: RandomForestClassifier(class_weight='balanced', max_depth=19, max_features=5,\n",
      "                       min_samples_leaf=3, min_samples_split=3, n_estimators=10,\n",
      "                       random_state=54321)\n",
      "\n",
      "Mejor valor f1 para bosque aleatorio: 0.5935984481086324\n"
     ]
    }
   ],
   "source": [
    "#bosque aleatorio\n",
    "best_model_forest = None\n",
    "f1_best_result_forest = 0.59\n",
    "\n",
    "for est in range(10, 80, 100):\n",
    "    for depth in range(1, 20):\n",
    "        model_forest = RandomForestClassifier(\n",
    "            random_state=54321, n_estimators=est, max_depth=depth, min_samples_leaf=3, \n",
    "            min_samples_split=3, max_features=5, class_weight='balanced'\n",
    "            )\n",
    "        model_forest.fit(features_train_upsampled_trees, target_train_upsampled_trees)\n",
    "        predicted_valid_forest = model_forest.predict(features_valid_ordinal)\n",
    "        \n",
    "        result = f1_score(target_valid_ordinal, predicted_valid_forest)\n",
    "\n",
    "        if result >= f1_best_result_forest:\n",
    "            best_model_forest = model_forest\n",
    "            f1_best_result_forest = result\n",
    "\n",
    "\n",
    "print('Mejor modelo de bosque aleatorio:', best_model_forest)\n",
    "print()\n",
    "print('Mejor valor f1 para bosque aleatorio:', f1_best_result_forest)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
